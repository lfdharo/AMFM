#settings .yaml
#author : ' vanmaren'
#description: yaml file for confirating paramaters for Training process
#interesting parameters:


calculatescores:
  MIN_COUNTS: &MIN_COUNTS 0  # Number of times a word must occur to be included in the SVM model
  MIN_NGRAM_ORDER: &MIN_NGRAM_ORDER 1
  MAX_NGRAM_ORDER: &MAX_NGRAM_ORDER 3
  NUM_MAX_CORES: &num_max_cores 4
  MIN_LOG_PROB: -10
  STARTING_VALUE_FEATURES: 25
  NFOLDS: &folds  1 # Number of NFOLDS cross-training sets we are creating for the AM experiments

filesPerLanguageForLM:
  'jp': ['JIJI_CORPUS/train']
  'en': [ 'JIJI_CORPUS/train']

#################################################################
directories:
  INPUT: /Escritorio/TFG_Pendrive/AMFM/AMFM_TFG2018/train/FILES_INPUT
  OUTPUT: /Escritorio/TFG_Pendrive/AMFM/AMFM_TFG2018/train/FILES_OUTPUT
#################################################################
runall:
 NFOLDS: *folds
 Preprocessing:
   Clean: True
   Lowercase: True
   Removepunctuation: True
   Tokenize: True #parameter for preprocessing with tokenizer
   TargetLang:
   SourceLang:
 dictSizesTrain:
    Maxvalue: &MAX_DICT_SIZE 15000 #total of lines in the training file
    Minvalue: &MIN_DICT_SIZE 2500  # FOR WAT2018-My-En (size of the  SVD)
 filesPerLanguageForLM: &filesPerLanguageForLM # we could use pointers * so every script has everything setted up correctly
    'jp': ['JIJI_CORPUS/train']
    'en': [ 'JIJI_CORPUS/train']
 filesPerLanguage: &filesPerLanguage
    'en-jp' : ['JIJI_CORPUS/train']
   # 'jp-en': ['JIJI_CORPUS/train']

#################################################################

preProcessModule:
  NFOLDS: *folds
  NUM_CORES_MAX: *num_max_cores
  aSizesTrain: *MAX_DICT_SIZE #ESTE numero ha de coincidir con runall:DicSizesTrain?????????? del runall?
  MIN_SENTENCE_LENGTH: 1
  MAX_SENTENCE_LENGTH: 50

  MIN_SENTENCE_LENGTH_CHARS: 1
  MAX_SENTENCE_LENGTH_CHARS: 150

  SCRIPTS_DIR: None
  TRAIN_DATA_DIR: None
  filesPerLanguageForLM: *filesPerLanguageForLM
  filesPerLanguage: *filesPerLanguage

  Tokenize : true #use the default tokenizing scripts (pearl)
  Tokenize_Script : '' #insert your script for tokenizing hear ( set Tokenize to false)

  attributes_per_language:
      en : 'words'
      jp: 'chars'

#################################################################

trainModule: #a√±adir flag para tokenizacion por word o charc
  NFOLDS: *folds
  MIN_COUNTS:  *MIN_COUNTS # Number of times a word must occur to be included in the SVM model
  MIN_NGRAM_ORDER: *MIN_NGRAM_ORDER
  MAX_NGRAM_ORDER: *MAX_NGRAM_ORDER
  NUM_MAX_CORES: *num_max_cores
  dictSizesTrain:
    Max: *MAX_DICT_SIZE
    Min: *MIN_DICT_SIZE  # For WAT2016-JA-HI
  num_cores_mono:
    Max: 2500
    Min: 5
  filesPerLanguageForLM: *filesPerLanguageForLM
  filesPerLanguage: *filesPerLanguage
################################################################# 
vesctor_space:




